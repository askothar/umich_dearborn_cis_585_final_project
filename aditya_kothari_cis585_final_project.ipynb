{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bcb993b",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models (DDPMs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876f8228",
   "metadata": {},
   "source": [
    "### Imports and Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa0041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import random\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Lambda, Resize, Grayscale\n",
    "from torchvision.datasets.mnist import MNIST, FashionMNIST\n",
    "from torchvision.datasets import Places365, Flowers102, Food101, CIFAR10\n",
    "\n",
    "# Definitions\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Path for storing the model\n",
    "#STORE_PATH_MNIST = f\"ddpm_model_mnist.pt\"\n",
    "#STORE_PATH_FASHION = f\"ddpm_model_fashion.pt\"\n",
    "#STORE_PATH_PLACES = f\"ddpm_model_places365.pt\"\n",
    "#STORE_PATH_FLOWERS = f\"ddpm_model_flowers102.pt\"\n",
    "#STORE_PATH_FOOD = f\"ddpm_model_food101.pt\"\n",
    "#STORE_PATH_CIFAR = f\"ddpm_model_cifar10.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64ba61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyper-parameters\n",
    "no_train = True\n",
    "dataset_loaded = 'flowers'\n",
    "batch_size = 28\n",
    "epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b004fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up store path, generated file name and dataset to load\n",
    "if dataset_loaded == 'fashion':\n",
    "    store_path = \"ddpm_model_fashion.pt\"\n",
    "    gif_name=\"fashion.gif\"\n",
    "    ds_fn = FashionMNIST\n",
    "elif dataset_loaded == 'places':\n",
    "    store_path = \"ddpm_model_places365.pt\"\n",
    "    gif_name=\"places.gif\"\n",
    "    ds_fn = Places365\n",
    "elif dataset_loaded == 'flowers':\n",
    "    store_path = \"ddpm_model_flowers102.pt\"\n",
    "    gif_name=\"flowers.gif\"\n",
    "    ds_fn = Flowers102\n",
    "elif dataset_loaded == 'food':\n",
    "    store_path = \"ddpm_model_food101.pt\"\n",
    "    gif_name=\"food.gif\"\n",
    "    ds_fn = Food101\n",
    "elif dataset_loaded == 'cifar':\n",
    "    store_path = \"ddpm_model_cifar10.pt\"\n",
    "    gif_name=\"cifar.gif\"\n",
    "    ds_fn = CIFAR10    \n",
    "else: \n",
    "    store_path = \"ddpm_mnist.pt\"\n",
    "    gif_name=\"mnist.gif\"\n",
    "    ds_fn = MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11901b85",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6b66ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show images\n",
    "def show_images(images, title=\"\"):\n",
    "    \"\"\"Shows the provided images as sub-pictures in a square\"\"\"\n",
    "\n",
    "    # Converting images to CPU numpy arrays\n",
    "    if type(images) is torch.Tensor:\n",
    "        images = images.detach().cpu().numpy()\n",
    "\n",
    "    # Defining number of rows and columns\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    rows = int(len(images) ** (1 / 2))\n",
    "    cols = round(len(images) / rows)\n",
    "\n",
    "    # Populating figure with sub-plots\n",
    "    idx = 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            fig.add_subplot(rows, cols, idx + 1)\n",
    "\n",
    "            if idx < len(images):\n",
    "                plt.imshow(images[idx][0], cmap=\"gray\")\n",
    "                idx += 1\n",
    "    fig.suptitle(title, fontsize=30)\n",
    "\n",
    "    # Showing the figure\n",
    "    plt.show()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f997aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show first batch of images\n",
    "def show_first_batch(loader):\n",
    "    for batch in loader:\n",
    "        show_images(batch[0], \"Images in the first batch\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a7af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data (converting each image into a tensor and normalizing between [-1, 1])\n",
    "transform = Compose([Resize([32,32]),\n",
    "    ToTensor(),                \n",
    "    Lambda(lambda x: (x - 0.5) * 2)]\n",
    ")\n",
    "dataset = ds_fn(\"./datasets\", download=True, transform=transform)\n",
    "loader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806a4e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the loaded first batch\n",
    "show_first_batch(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511557b9",
   "metadata": {},
   "source": [
    "### DDPM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff99e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\t\" + (f\"{torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"CPU\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a17c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPM class\n",
    "class MyDDPM(nn.Module):\n",
    "    def __init__(self, network, n_steps=200, min_beta=10 ** -4, max_beta=0.02, device=None, image_chw=(3, 32, 32)):\n",
    "        super(MyDDPM, self).__init__()\n",
    "        self.n_steps = n_steps\n",
    "        self.device = device\n",
    "        self.image_chw = image_chw\n",
    "        self.network = network.to(device)\n",
    "        self.betas = torch.linspace(min_beta, max_beta, n_steps).to(device)  # Number of steps is typically in the order of thousands\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.tensor([torch.prod(self.alphas[:i + 1]) for i in range(len(self.alphas))]).to(device)\n",
    "\n",
    "    def forward(self, x0, t, eta=None):\n",
    "        # Make input image more noisy (we can directly skip to the desired step)\n",
    "        n, c, h, w = x0.shape\n",
    "        a_bar = self.alpha_bars[t]\n",
    "\n",
    "        if eta is None:\n",
    "            eta = torch.randn(n, c, h, w).to(self.device)\n",
    "\n",
    "        noisy = a_bar.sqrt().reshape(n, 1, 1, 1) * x0 + (1 - a_bar).sqrt().reshape(n, 1, 1, 1) * eta\n",
    "        return noisy\n",
    "\n",
    "    def backward(self, x, t):\n",
    "        # Run each image through the network for each timestep t in the vector t.\n",
    "        # The network returns its estimation of the noise that was added.\n",
    "        return self.network(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50058145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the forward process\n",
    "def show_forward(ddpm, loader, device):    \n",
    "    for batch in loader:\n",
    "        imgs = batch[0]\n",
    "\n",
    "        show_images(imgs, \"Original images\")\n",
    "\n",
    "        for percent in [0.25, 0.5, 0.75, 1]:\n",
    "            show_images(\n",
    "                ddpm(imgs.to(device),\n",
    "                     [int(percent * ddpm.n_steps) - 1 for _ in range(len(imgs))]),\n",
    "                f\"DDPM Noisy images {int(percent * 100)}%\"\n",
    "            )\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfacf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new image samples for a given DDPM model, a given number of samples to be generated and a given device\n",
    "def generate_new_images(ddpm, n_samples=16, device=None, frames_per_gif=100, gif_name=\"sampling.gif\", c=3, h=32, w=32):\n",
    "    frame_idxs = np.linspace(0, ddpm.n_steps, frames_per_gif).astype(np.uint)\n",
    "    frames = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device is None:\n",
    "            device = ddpm.device\n",
    "            \n",
    "        # Starting from random noise\n",
    "        x = torch.randn(n_samples, c, h, w).to(device)\n",
    "        \n",
    "        for idx, t in enumerate(list(range(ddpm.n_steps))[::-1]):\n",
    "            # Estimating noise to be removed\n",
    "            time_tensor = (torch.ones(n_samples,1)*t).to(device).long()\n",
    "            eta_theta = ddpm.backward(x, time_tensor)\n",
    "            alpha_t = ddpm.alphas[t]\n",
    "            alpha_t_bar = ddpm.alpha_bars[t]\n",
    "            \n",
    "            # Partially denoising the image\n",
    "            x = (1/alpha_t.sqrt())*(x-(1-alpha_t)/(1-alpha_t_bar).sqrt()*eta_theta)\n",
    "            \n",
    "            if t>0:\n",
    "                z = torch.randn(n_samples, c, h, w).to(device)\n",
    "                \n",
    "                # sigma_t squared = beta_t\n",
    "                beta_t = ddpm.betas[t]\n",
    "                sigma_t = beta_t.sqrt()\n",
    "                \n",
    "                # Adding some more noise like in Langevin Dynamics fashion\n",
    "                x = x + sigma_t * z\n",
    "                \n",
    "            # Adding frames to the GIF\n",
    "            if idx in frame_idxs or t==0:\n",
    "                # Putting digits in range [0,255]\n",
    "                normalized = x.clone()\n",
    "                for i in range(len(normalized)):\n",
    "                    normalized[i] -= torch.min(normalized[i])\n",
    "                    normalized[i] *= 255/torch.max(normalized[i])\n",
    "            \n",
    "                # Reshaping batch (n, c, h, w) to be a square frame\n",
    "                frame = einops.rearrange(normalized, \"(b1 b2) c h w -> (b1 h) (b2 w) c\", b1=int(n_samples ** 0.5))\n",
    "                frame = frame.cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "                # Rendering frame\n",
    "                frames.append(frame)\n",
    "                \n",
    "    # Storing the gif\n",
    "    with imageio.get_writer(gif_name, mode='I') as writer:\n",
    "        for idx, frame in enumerate(frames):\n",
    "            writer.append_data(frame)\n",
    "            if idx == len(frames) - 1:\n",
    "                for _ in range(frames_per_gif//3):\n",
    "                    writer.append_data(frames[-1])\n",
    "                    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b167df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return the standard positional embedding\n",
    "def sinusoidal_embedding(n, d):\n",
    "    embedding = torch.tensor([[i / 10000 ** (2*j/d) for j in range(d)] for i in range(n)])\n",
    "    sin_mask = torch.arange(0, n, 2)\n",
    "    embedding[sin_mask] = torch.sin(embedding[sin_mask])\n",
    "    embedding[1 - sin_mask] = torch.cos(embedding[sin_mask])\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e302b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block class for convolution\n",
    "class MyBlock(nn.Module):\n",
    "    def __init__(self, shape, in_c, out_c, kernel_size=3, stride=1, padding=1, activation=None, normalize=True):\n",
    "        super(MyBlock, self).__init__()\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.gn1 = nn.GroupNorm(num_groups=1, num_channels=out_c)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size, stride, padding)\n",
    "        self.gn2 = nn.GroupNorm(num_groups=1, num_channels=out_c)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size, stride, padding)\n",
    "        self.activation = nn.GELU() if activation is None else activation\n",
    "        self.normalize = normalize\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #out = self.ln(x) if self.normalize else x        \n",
    "        out = self.conv1(x)\n",
    "        out = self.gn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.gn2(out)\n",
    "        out = self.activation(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd7437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self-attestation class for UNet\n",
    "class MyTESA(nn.Module):\n",
    "    def __init__(self, in_c, size, num_heads=4, hidden_dim=1024, dropout=0.0):\n",
    "        super(MyTESA, self).__init__()\n",
    "        self.in_c = in_c\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=in_c, num_heads=num_heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm([in_c])\n",
    "        self.ln2 = nn.LayerNorm([in_c])\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=in_c, out_features=hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=in_c),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.in_c, self.size*self.size).permute(0,2,1)\n",
    "        out = self.ln1(x)\n",
    "        attention_value, _ = self.mha(query=out, key=out, value=out)\n",
    "        out = attention_value + x\n",
    "        out = self.mlp(self.ln2(out))+out\n",
    "        return out.permute(0,2,1).reshape(-1, self.in_c, self.size, self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1344716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unet class\n",
    "class MyUNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_steps=500, time_emb_dim=256):\n",
    "        super(MyUNet, self).__init__()\n",
    "        \n",
    "        # Sinusoidal embedding\n",
    "        self.time_embed = nn.Embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.weight.data = sinusoidal_embedding(n_steps, time_emb_dim)\n",
    "        self.time_embed.requires_grad_(False)\n",
    "        \n",
    "        # First half\n",
    "        self.input_conv = MyBlock((n_channels,32,32),n_channels,64)\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            MyBlock((64,16,16),64,64),\n",
    "            MyBlock((64,16,16),64,128)\n",
    "        )\n",
    "        self.te1 = self._make_te(time_emb_dim,64)\n",
    "        self.sa1 = MyTESA(128,16)\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            MyBlock((128,8,8),128,128),\n",
    "            MyBlock((128,8,8),128,256)\n",
    "        )\n",
    "        self.te2 = self._make_te(time_emb_dim,128)\n",
    "        self.sa2 = MyTESA(256,8)\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            MyBlock((256,4,4),256,256),\n",
    "            MyBlock((256,4,4),256,256)\n",
    "        )\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.te3 = self._make_te(time_emb_dim,256)\n",
    "        self.sa3 = MyTESA(256,4)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            MyBlock((256,4,4),256,512),\n",
    "            MyBlock((512,4,4),512,512),\n",
    "            MyBlock((512,4,4),512,256)\n",
    "        )\n",
    "        \n",
    "        # Second half\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up1_conv = nn.Sequential(\n",
    "            MyBlock((512,8,8),512,256),\n",
    "            MyBlock((256,8,8),256,128)\n",
    "        )\n",
    "        self.te4 = self._make_te(time_emb_dim,512)\n",
    "        self.sa4 = MyTESA(128,8)\n",
    "        self.up2_conv = nn.Sequential(\n",
    "            MyBlock((256,16,16),256,128),\n",
    "            MyBlock((128,16,16),128,64)\n",
    "        )\n",
    "        self.te5 = self._make_te(time_emb_dim,256)\n",
    "        self.sa5 = MyTESA(64,16)\n",
    "        self.up3_conv = nn.Sequential(\n",
    "            MyBlock((128,32,32),128,64),\n",
    "            MyBlock((64,32,32),64,64)\n",
    "        )\n",
    "        self.te6 = self._make_te(time_emb_dim,128)\n",
    "        self.sa6 = MyTESA(64,32)\n",
    "        self.out_conv = nn.Conv2d(64, n_channels, (1, 1))\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        t = self.time_embed(t)\n",
    "        n = len(x)\n",
    "        out1 = self.input_conv(x)\n",
    "        out2 = self.down1(out1 + self.te1(t).reshape(n, -1, 1, 1))\n",
    "        out2 = self.sa1(out2)\n",
    "        out3 = self.down2(out2 + self.te2(t).reshape(n, -1, 1, 1))\n",
    "        out3 = self.sa2(out3)\n",
    "        out4 = self.down3(out3 + self.te3(t).reshape(n, -1, 1, 1))\n",
    "        out4 = self.sa3(out4)\n",
    "        out5 = self.bottleneck(out4)\n",
    "        out6 = self.up1_conv(torch.cat((out3, self.up(out5)), dim=1) + self.te4(t).reshape(n, -1, 1, 1))\n",
    "        out6 = self.sa4(out6)\n",
    "        out7 = self.up2_conv(torch.cat((out2, self.up(out6)), dim=1) + self.te5(t).reshape(n, -1, 1, 1))\n",
    "        out7 = self.sa5(out7)\n",
    "        out8 = self.up3_conv(torch.cat((out1, self.up(out7)), dim=1) + self.te6(t).reshape(n, -1, 1, 1))\n",
    "        out8 = self.sa6(out8)\n",
    "        return self.out_conv(out8)\n",
    "    \n",
    "    def _make_te(self, dim_in, dim_out):\n",
    "        return nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(dim_in, dim_out)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb25f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the DDPM model\n",
    "n_channels, n_steps, min_beta, max_beta = 1, 500, 10**-4, 0.02\n",
    "ddpm = MyDDPM(MyUNet(n_channels, n_steps), n_steps=n_steps, min_beta=min_beta, max_beta=max_beta, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e8e47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_forward(ddpm, loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5677c7",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5afd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion for training loop\n",
    "def training_loop(ddpm, loader, n_epochs, optim, device, display=False, store_path=\"ddpm_model.pt\"):\n",
    "    mse = nn.MSELoss()\n",
    "    smoothL1 = nn.SmoothL1Loss()\n",
    "    best_loss = float(\"inf\")\n",
    "    n_steps = ddpm.n_steps\n",
    "    for epoch in tqdm(range(n_epochs), desc=f\"Training progress\", colour=\"#00ff00\"):\n",
    "        epoch_loss = 0.\n",
    "        for step, batch in enumerate(tqdm(loader, leave=False, desc=f\"Epoch {epoch+1}/{n_epochs}\", colour=\"#005500\")):\n",
    "            # Loading data\n",
    "            x0 = batch[0].to(device)\n",
    "            n = len(x0)\n",
    "            \n",
    "            # Picking some noise for each of the images in the batch, a timestep and the respective alpha_bars\n",
    "            eta = torch.randn_like(x0).to(device)\n",
    "            t = torch.randint(0, n_steps, (n,)).to(device)\n",
    "            \n",
    "            # Computing the noisy image based on x0 and the time-step (forward process)\n",
    "            noisy_imgs = ddpm(x0, t, eta)\n",
    "\n",
    "            # Getting model estimation of noise based on the images and the time-step\n",
    "            eta_theta = ddpm.backward(noisy_imgs, t.reshape(n,-1))\n",
    "            \n",
    "            # Optimizing the Smooth L1 between the noise plugged and the predicted noise\n",
    "            loss = smoothL1(eta_theta, eta)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            epoch_loss += loss.item()*len(x0)/len(loader.dataset)\n",
    "            \n",
    "        # Display images generated at this epoch\n",
    "        if display:\n",
    "            show_images(generate_new_images(ddpm, device=device), f\"Images generated at epoch {epoch+1}\")\n",
    "            \n",
    "        log_string = f\"Loss at epoch {epoch+1}: {epoch_loss:.3f}\"\n",
    "        \n",
    "        # Storing the model\n",
    "        if best_loss > epoch_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(ddpm.state_dict(), store_path)\n",
    "            log_string += \"--> Best model ever (stored)\"\n",
    "            \n",
    "        print(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1e579",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "if not no_train:\n",
    "    training_loop(ddpm, loader, epochs, optim=Adam(ddpm.parameters(), learning_rate), device=device, store_path=store_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028b5b6",
   "metadata": {},
   "source": [
    "### Generating New Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the trained model\n",
    "best_model = MyDDPM(MyUNet(n_channels), n_steps=n_steps, device=device)\n",
    "best_model.load_state_dict(torch.load(store_path, map_location=device))\n",
    "best_model.eval()\n",
    "print(\"Model loaded: Generating new images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe01a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generating new images\n",
    "generated = generate_new_images(\n",
    "        best_model,\n",
    "        n_samples=49,\n",
    "        device=device,\n",
    "        gif_name= gif_name,\n",
    "        c = n_channels\n",
    "    )\n",
    "show_images(generated, \"Final result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a39c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(open(gif_name,'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87279d",
   "metadata": {},
   "source": [
    "### Empty the GPU cache for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19525cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a18cf6",
   "metadata": {},
   "source": [
    "### Number of model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596e275c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#total_params = sum(\n",
    "#\tparam.numel() for param in best_model.parameters()\n",
    "#)\n",
    "#total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5af06f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainable_params = sum(\n",
    "#\tp.numel() for p in best_model.parameters() if p.requires_grad\n",
    "#)\n",
    "#trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f479a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
